{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6954a744-b3c7-4c91-8b51-4d6edd948d17",
   "metadata": {},
   "source": [
    "# API\n",
    "\n",
    "> This module defines the API of *findmycells*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133610c-c166-47e1-8923-d0b19fe65043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d254e8-c8ec-464e-9e20-14e3003ce2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path, PosixPath\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from findmycells.preprocessing.specs import PreprocessingStrategy, PreprocessingObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472d628-c4b0-40a3-9ecb-52626f2b3ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class API:\n",
    "    \n",
    "    def __init__(self, project_root_dir: PosixPath,\n",
    "                 project_configs_filepath: Optional[PosixPath]=None,\n",
    "                 database_filepath: Optional[PosixPath]=None) -> None:\n",
    "        assert type(project_root_dir) == PosixPath, '\"project_root_dir\" must be pathlib.Path object referring to an existing directory.'\n",
    "        assert project_root_dir.is_dir(), '\"project_root_dir\" must be pathlib.Path object referring to an existing directory.'\n",
    "        if project_configs_filepath != None:\n",
    "            assert type(project_configs_filepath) == PosixPath, '\"project_configs_filepath\" must be pathlib.Path object referring to a .config file.'\n",
    "            assert project_configs_filepath.suffix == '.config', '\"project_configs_filepath\" must be pathlib.Path object referring to a .config file.'\n",
    "        if database_filepath != None:\n",
    "            assert type(database_filepath) == PosixPath, '\"database_filepath\" must be pathlib.Path object referring to a .obj file'\n",
    "            assert database_filepath.suffix == '.obj', '\"database_filepath\" must be pathlib.Path object referring to a .obj file'\n",
    "        self.project_configs = ProjectConfigs(project_root_dir = project_root_dir, project_configs_filepath = project_configs_filepath)\n",
    "        self.database = Database(project_configs = project_configs, database_filepath = database_filepath)\n",
    "        \n",
    "        \n",
    "    def update_database_with_current_source_files(self, skip_checking: bool=False) -> None:\n",
    "        self.database.compute_file_infos(skip_checking = skip_checking)\n",
    "        \n",
    "        \n",
    "    def preprocess(self,\n",
    "                   strategies: List[PreprocessingStrategy],\n",
    "                   strategy_configs: Optional[List[Dict]]=None,\n",
    "                   processing_configs: Optional[Dict]=None,\n",
    "                   file_ids: Optional[List[str]]=None\n",
    "                  ) -> None:\n",
    "        processing_step_id = 'preprocessing'\n",
    "        strategy_configs, processing_configs, file_ids = self._assert_and_update_input(processing_step_id = processing_step_id,\n",
    "                                                                                       strategies = strategies,\n",
    "                                                                                       strategy_configs = strategy_configs,\n",
    "                                                                                       processing_configs = processing_configs,\n",
    "                                                                                       file_ids = file_ids)\n",
    "        for file_id in tqdm(file_ids, display = processing_configs['show_progress']):\n",
    "            preprocessing_object = PreprocessingObject()\n",
    "            preprocessing_object.prepare_for_processing(file_ids = [file_id], database = self.database)\n",
    "            preprocessing_object.load_image_and_rois()\n",
    "            preprocessing_object.run_all_strategies(strategies = strategies, strategy_configs = strategy_configs)\n",
    "            preprocessing_object.save_preprocessed_images_on_disk()\n",
    "            preprocessing_object.save_preprocessed_rois_in_database()\n",
    "            preprocessing_object.update_database()\n",
    "            del preprocessing_object\n",
    "            if processing_configs['autosave'] == True:\n",
    "                self.save_status()\n",
    "                \n",
    "    \n",
    "    def save_status(self): -> None:\n",
    "        self.project_configs.save_to_disk()\n",
    "        self.database.save_to_disk()\n",
    "        \n",
    "        \n",
    "    def load_status(self,\n",
    "                    project_configs_filepath: Optional[PosixPath],\n",
    "                    database_filepath: Optional[PosixPath]\n",
    "                   ) -> None:\n",
    "        if project_configs_filepath != None:\n",
    "            assert type(project_configs_filepath) == PosixPath, '\"project_configs_filepath\" must be pathlib.Path object referring to a .config file.'\n",
    "            assert project_configs_filepath.suffix == '.config', '\"project_configs_filepath\" must be pathlib.Path object referring to a .config file.'\n",
    "        else:\n",
    "            project_configs_filepath = self._look_for_latest_status_filepath(suffix = '.config', dir_path = self.root_dir)\n",
    "        if database_filepath != None:\n",
    "            assert type(database_filepath) == PosixPath, '\"database_filepath\" must be pathlib.Path object referring to a .obj file'\n",
    "            assert database_filepath.suffix == '.obj', '\"database_filepath\" must be pathlib.Path object referring to a .obj file'\n",
    "        else:\n",
    "            database_filepath = self._look_for_latest_status_filepath(suffix = '.obj', dir_path = self.root_dir.joinpath('results'))\n",
    "        self.project_configs.load_from_disk(project_configs_filepath = project_configs_filepath)\n",
    "        self.database.load_from_disk(database_filepath = database_filepath, project_configs = self.project_configs)        \n",
    "        \n",
    "\n",
    "    def split_file_ids_into_batches(self, file_ids: List[str], batch_size: int) -> List[List[str]]:\n",
    "        if len(file_ids) % batch_size == 0:\n",
    "            total_batches = int(len(file_ids) / batch_size)\n",
    "        else:\n",
    "            total_batches = int(len(file_ids) / batch_size) + 1\n",
    "        file_ids_per_batch = []\n",
    "        for batch in range(total_batches):\n",
    "            if len(file_ids) >= batch_size:\n",
    "                sampled_file_ids = random.sample(file_ids, batch_size)\n",
    "            else:\n",
    "                sampled_file_ids = file_ids.copy()\n",
    "            file_ids_per_batch.append(sampled_file_ids)\n",
    "            for elem in sampled_file_ids:\n",
    "                file_ids.remove(elem)    \n",
    "        return file_ids_per_batch\n",
    "\n",
    "\n",
    "    def _look_for_latest_status_file_in_dir(self, suffix: str, dir_path: PosixPath) -> PosixPath:\n",
    "        matching_filepaths = [filepath for filepath in dir_path.iter_dir() if filepath.suffix == suffix]\n",
    "        if len(matching_filepaths) == 0:\n",
    "            raise FileNotFoundError(f'Could not find a \"{suffix}\" file in {dir_path}. Consider specifying the exact filepath!')\n",
    "        else:\n",
    "            date_strings = [filepath[:10] for filepath in matching_filepaths]\n",
    "            dates = [datetime.strptime(date_str, '%Y_%m_%d') for date_str in date_strings]\n",
    "            latest_date = max(dates)\n",
    "            filepath_idx = dates.index(latest_date)\n",
    "            latest_status_filepath = matching_filepaths[filepath_idx]\n",
    "        return latest_status_filepath        \n",
    "        \n",
    "        \n",
    "    def _assert_and_update_input(self, \n",
    "                                 processing_step_id: str,\n",
    "                                 strategies: List[PreprocessingStrategy],\n",
    "                                 strategy_configs: Optional[List[Dict]],\n",
    "                                 processing_configs: Optional[Dict],\n",
    "                                 file_ids: Optional[List[str]]\n",
    "                                ) -> Tuple[List[Dict], Dict, List[str]]:\n",
    "        self._assert_processing_step_input(processing_step_id = processing_step_id,\n",
    "                                           strategies = strategies,\n",
    "                                           strategy_configs = strategy_configs,\n",
    "                                           processing_configs = processing_configs,\n",
    "                                           file_ids = file_ids)\n",
    "        strategy_configs = self._fill_strategy_configs_with_defaults_where_needed(strategies, strategy_configs)\n",
    "        if processing_configs == None:\n",
    "            processing_configs = getattr(self.project_configs, processing_step_id)\n",
    "        processing_configs = self._fill_processing_configs_with_defaults_where_needed(processing_step_id, processing_configs)\n",
    "        self.project_configs.add_processing_step_configs(processing_step_id, configs = processing_configs)\n",
    "        file_ids = self.database.get_file_ids_to_process(input_file_ids = file_ids,\n",
    "                                                         processing_step_id = processing_step_id,\n",
    "                                                         overwrite = processing_configs['overwrite'])\n",
    "        return strategy_configs, processing_configs, file_ids\n",
    "        \n",
    "            \n",
    "        \n",
    "    def _assert_processing_step_input(self, \n",
    "                                      processing_step_id: str,\n",
    "                                      strategies: List[PreprocessingStrategy],\n",
    "                                      strategy_configs: Optional[List[Dict]],\n",
    "                                      processing_configs: Optional[Dict],\n",
    "                                      file_ids: Optional[List[str]]\n",
    "                                     ) -> None:\n",
    "        assert type(strategies) == list, '\"strategies\" has to ba a list of ProcessingStrategy classes of the respective processing step!'\n",
    "        if strategy_configs != None:\n",
    "            assert type(strategy_configs) == list, '\"strategy_configs\" has to be None or a list of the same length as \"strategies\"!'\n",
    "            assert len(strategy_configs) == len(strategies), '\"strategy_configs\" has to be None or a list of the same length as \"strategies\"!'\n",
    "        else:\n",
    "            strategy_configs = [None] * len(strategies)\n",
    "        available_strategies = self.project_configs.available_processing_strategies[processing_step_id]\n",
    "        for strat, config in zip(strategies, strategy_configs):\n",
    "            assert strat in available_strategies, f'{strat} is not an available strategy for {processing_step_id}!'\n",
    "            if config != None:\n",
    "                strat().default_configs.assert_user_input(user_input = config)\n",
    "        if processing_configs != None:\n",
    "            processing_obj = self.project_configs.available_processing_objects[processing_step_id]()\n",
    "            processing_obj.default_configs.assert_user_input(user_input = processing_configs)\n",
    "        if file_ids != None:\n",
    "            assert type(file_ids) == list, '\"file_ids\" has to be a list of strings referring to file_ids in the database!'\n",
    "            for elem in file_ids:\n",
    "                assert elem in self.database.file_infos['file_id'], f'{elem} is not a valid file_id!'\n",
    "        \n",
    "        \n",
    "    def _fill_processing_configs_with_defaults_where_needed(self,\n",
    "                                                            processing_step_id: str,\n",
    "                                                            processing_configs: Dict\n",
    "                                                           ) -> Dict:\n",
    "        processing_obj = self.project_configs.available_processing_objects[processing_step_id]()\n",
    "        return processing_obj.default_configs.fill_user_input_with_defaults_where_needed(user_input = processing_configs)                                              \n",
    "             \n",
    "        \n",
    "    def _fill_strategy_configs_with_defaults_where_needed(self,\n",
    "                                                          strategies: List[ProcessingStrategy],\n",
    "                                                          strategy_configs: Optional[List[Dict]]\n",
    "                                                         ) -> List[Dict]:\n",
    "        all_final_configs = []\n",
    "        if strategy_configs == None:\n",
    "            strategy_configs = [{}] * len(strategies)\n",
    "        for strat, configs in zip(strategies, strategy_configs):\n",
    "            full_configs = strat().default_configs.fill_user_input_with_defaults_where_needed(user_input = configs)\n",
    "            all_final_configs.append(full_configs)\n",
    "        return all_final_configs            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6a110-9089-4453-8769-437b9da3ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class refactored_Project:\n",
    "\n",
    "    def __init__(self, project_root_dir: Path, project_configs_filepath: Optional[Path]=None) -> None:\n",
    "        self.project_configs = ProjectConfigs(project_root_dir = project_root_dir, project_configs_filepath = project_configs_filepath)\n",
    "        self.project_root_dir = project_root_dir\n",
    "        self.database = Database(project_configs = project_configs)\n",
    "\n",
    "\n",
    "    def load_microscopy_image_filepaths(self) -> None:\n",
    "        # If microscopy images are already present --> retrieve filepaths & group / subject / condition metadata\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def load_roi_filepaths(self) -> None:\n",
    "        # If roi files are already present --> retrieve filepaths & ensure that they match with the microscopy image filepaths\n",
    "        # Since ROIs should be optionally (ideally, since whole image analysis should also be possible) don´t break if missing\n",
    "        pass\n",
    "\n",
    "\n",
    "    def load_status(self, project_configs_filepath: Path) -> None:\n",
    "        raise NotImplementedError()\n",
    "        self.database.load_all()\n",
    "        self.project_configs.attempt_loading_from_configs_filepath(project_configs_filepath = project_configs_filepath)\n",
    "        \n",
    "\n",
    "    def preprocess(self,\n",
    "                   strategies: List[PreprocessingStrategy],\n",
    "                   strategy_configs: Optional[List[Dict]]=None,\n",
    "                   processing_configs: Optional[Dict]=None,\n",
    "                   file_ids: Optional[List[str]]=None\n",
    "                  ) -> None:\n",
    "        updated_inputs = self._validate_input_and_prepare_processing_step(strategies = strategies,\n",
    "                                                                          strategy_configs = strategy_configs,\n",
    "                                                                          processing_configs = processing_configs,\n",
    "                                                                          file_ids = file_ids,\n",
    "                                                                          processing_object = PreprocessingObject,\n",
    "                                                                          processing_type = 'preprocessing')\n",
    "        updated_processing_configs, updated_strategy_configs, updated_file_ids = updated_inputs\n",
    "        for file_id in updated_file_ids:\n",
    "            preprocessing_object = PreprocessingObject(file_ids = [file_id], database = self.database, project_configs = self.project_configs)\n",
    "            preprocessing_object.load_image_and_rois()\n",
    "            preprocessing_object.run_all_strategies(strategies = strategies, strategy_configs = updated_strategy_configs)\n",
    "            preprocessing_object.save_preprocessed_images_on_disk()\n",
    "            preprocessing_object.save_preprocessed_rois_in_database()\n",
    "            preprocessing_object.update_database()\n",
    "            del preprocessing_object\n",
    "            \n",
    "    \n",
    "    def save_status(self) -> None:\n",
    "        raise NotImplementedError()\n",
    "        self.database.save_all()\n",
    "        self.project_configs.export_as_yml()\n",
    "       \n",
    "        \n",
    "    def _assert_valid_input_for_processing_methods(self, \n",
    "                                                   strategies: List[ProcessingStrategy],\n",
    "                                                   strategy_configs: Optional[List[Dict]],\n",
    "                                                   processing_configs: Optional[Dict],\n",
    "                                                   file_ids: Optional[List[str]],\n",
    "                                                   processing_type: str\n",
    "                                                  ) -> None:\n",
    "        self._assert_types_for_all_elements_in_list(list_to_check = strategies,\n",
    "                                                    allow_none_instead_of_list = False,\n",
    "                                                    allowed_types = [ProcessingStrategy],\n",
    "                                                    id_for_error = 'strategies')\n",
    "        self._assert_types_for_all_elements_in_list(list_to_check = file_ids,\n",
    "                                                    allow_none_instead_of_list = True,\n",
    "                                                    allowed_types = [str],\n",
    "                                                    id_for_error = 'file IDs')        \n",
    "        self._assert_types_for_all_elements_in_list(list_to_check = strategy_configs,\n",
    "                                                    allow_none_instead_of_list = True,\n",
    "                                                    allowed_types = [Dict],\n",
    "                                                    id_for_error = 'strategy configs')\n",
    "        if type(strategy_configs) == list:\n",
    "            assert len(strategies) != len(strategy_configs), 'You must provide exactly as many strategy config dictionaries as ProcessingStrategy objects!'\n",
    "        for strategy in strategies:\n",
    "            assert strategy().processing_type == processing_type, f'Not all processing strategies are intended to be used for this step!'\n",
    "        assert type(processing_configs) not in [dict, type(None)], '\"processing_configs\" was neither a dictionary nor None.'\n",
    "        # assert that all file_ids exist       \n",
    "\n",
    "\n",
    "    def _assert_types_for_all_elements_in_list(self, \n",
    "                                               list_to_check: Optional[List[Any]], \n",
    "                                               allow_none_instead_of_list: bool, \n",
    "                                               allowed_types: List[type], \n",
    "                                               id_for_error: str\n",
    "                                              ) -> None:\n",
    "        if allow_none_instead_of_list == True:\n",
    "            assert type(list_to_check) in [list, type(None)], f'Neither a list nor \"None\" was provided, but a {type(list_to_check)} instead. Identifier for Traceback: {id_for_error}.'\n",
    "        else:\n",
    "            assert type(list_to_check) == list, f'\"list_to_check\" is actually not a list! Identifier for Traceback: {id_for_error}.'\n",
    "        if list_to_check =! None:\n",
    "            for elem in list_to_check:\n",
    "                assert type(elem) in allowed_types, f'Found {type(elem)} in a list where {allowed_types} are allowed. Identifier for Traceback: {id_for_error}.'\n",
    "            \n",
    "        \n",
    "    def _validate_input_and_prepare_processing_step(self,\n",
    "                                                     strategies: List[ProcessingStrategy],\n",
    "                                                     strategy_configs: Optional[List[Dict]],\n",
    "                                                     processing_configs: Optional[Dict],\n",
    "                                                     file_ids: Optional[List[str]],\n",
    "                                                     processing_object_class: ProcessingObject,\n",
    "                                                     processing_type: str\n",
    "                                                    ) -> Tuple[Dict, List[Dict], List[str]]:\n",
    "        self._assert_valid_input_for_processing_methods(strategies = strategies,\n",
    "                                                        strategy_configs = strategy_configs\n",
    "                                                        processing_configs = processing_configs,\n",
    "                                                        file_ids = file_ids,\n",
    "                                                        processing_type = processing_type)\n",
    "        updated_processing_configs = self.project_configs.set_processing_type_specific_configs(processing_object_class = processing_object_class, \n",
    "                                                                                               processing_type_specific_configs = processing_configs)\n",
    "        updated_strategy_configs = []\n",
    "        if type(strategy_configs) == list:\n",
    "            for strategy, configs in zip(strategies, strategy_configs):\n",
    "                validated_configs = self.project_configs.set_strategy_specific_configs(strategy = strategy, strategy_configs = configs)\n",
    "                updated_strategy_configs.append(validated_configs)\n",
    "        else:\n",
    "            for strategy in strategies:\n",
    "                validated_configs = self.project_configs.set_strategy_specific_configs(strategy = strategy, strategy_configs = None)\n",
    "                updated_strategy_configs.append(validated_configs)\n",
    "        updated_file_ids = self.database.get_file_ids_to_process(requested_file_ids = file_ids, \n",
    "                                                                 processing_type = processing_type, \n",
    "                                                                 overwrite = updated_processing_configs['overwrite'])\n",
    "        return updated_processing_configs, updated_strategy_configs, updated_file_ids\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba7017-b86a-45c7-bd3c-f80fdf8f6b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ee4fe-7b51-4af0-8afb-087d46bbc9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Project:\n",
    "\n",
    "    def __init__(self, user_input: Dict):\n",
    "        self.project_root_dir = user_input['project_root_dir']\n",
    "        self.database = Database(user_input)\n",
    "\n",
    "\n",
    "    def save_status(self) -> None:\n",
    "        self.database.save_all()\n",
    "\n",
    "\n",
    "    def load_status(self) -> None:\n",
    "        self.database.load_all()\n",
    "\n",
    "\n",
    "    def preprocess(self, strategies: List[PreprocessingStrategy], file_ids: Optional[List]=None, overwrite: bool=False) -> None:\n",
    "        file_ids = self.database.get_file_ids_to_process(input_file_ids = file_ids, process_tracker_key = 'preprocessing_completed', overwrite = overwrite)\n",
    "        for file_id in file_ids:\n",
    "            preprocessing_object = PreprocessingObject(database = self.database, file_ids = [file_id], strategies = strategies)\n",
    "            preprocessing_object.run_all_strategies()\n",
    "            preprocessing_object.save_preprocessed_images_on_disk()\n",
    "            preprocessing_object.save_preprocessed_rois_in_database()\n",
    "            preprocessing_object.update_database()\n",
    "            del preprocessing_object\n",
    "            \n",
    "\n",
    "    def segment(self, strategies: List[SegmentationStrategy], file_ids: Optional[List]=None, batch_size: Optional[int]=None,\n",
    "                run_strategies_individually: bool=True, overwrite: bool=False, autosave: bool=True, clear_tmp_data: bool=False) -> None:\n",
    "        # check if there is a new strategy - if yes: reset \"segmentation_completed\" for all files to \"None\"\n",
    "        self.database.file_infos['segmentation_completed'] = self.reset_file_infos_if_new_strategy(strategies = strategies)\n",
    "        \n",
    "        if type(batch_size) == int:\n",
    "            file_ids_per_batch = self.create_batches(batch_size = batch_size, file_ids = file_ids, process_tracker_key = 'segmentation_completed', overwrite = overwrite)\n",
    "            if file_ids_per_batch == None:\n",
    "                return None\n",
    "        else:\n",
    "            file_ids_per_batch = [file_ids]\n",
    "        \n",
    "        if run_strategies_individually:\n",
    "            for segmentation_strategy in strategies:\n",
    "                for batch_file_ids in file_ids_per_batch:\n",
    "                    tracker = f'{segmentation_strategy().segmentation_type}_segmentations_done'\n",
    "                    tmp_file_ids = self.database.get_file_ids_to_process(input_file_ids = batch_file_ids, process_tracker_key = tracker, overwrite = overwrite)\n",
    "                    if len(tmp_file_ids) > 0:\n",
    "                        segmentation_object = SegmentationObject(database = self.database, file_ids = tmp_file_ids, strategies = [segmentation_strategy])\n",
    "                        segmentation_object.run_all_strategies()\n",
    "                        del segmentation_object\n",
    "                        if autosave:\n",
    "                            self.database.save_all()\n",
    "            if file_ids_per_batch[0] == None:\n",
    "                all_file_ids = None\n",
    "            else:\n",
    "                all_file_ids = []\n",
    "                for batch_file_ids in file_ids_per_batch:\n",
    "                    all_file_ids += batch_file_ids\n",
    "            all_file_ids = self.database.get_file_ids_to_process(input_file_ids = all_file_ids, process_tracker_key = 'segmentation_completed', overwrite = overwrite)\n",
    "            if len(all_file_ids) > 0:\n",
    "                segmentation_object = SegmentationObject(database = self.database, file_ids = all_file_ids, strategies = strategies)\n",
    "                segmentation_object.update_database()\n",
    "                del segmentation_object\n",
    "                if autosave:\n",
    "                    self.database.save_all()\n",
    "        else:\n",
    "            for batch_file_ids in file_ids_per_batch:\n",
    "                batch_file_ids = self.database.get_file_ids_to_process(input_file_ids = batch_file_ids, process_tracker_key = 'segmentation_completed', overwrite = overwrite)\n",
    "                segmentation_object = SegmentationObject(database = self.database, file_ids = batch_file_ids, strategies = strategies)\n",
    "                segmentation_object.run_all_strategies()\n",
    "                segmentation_object.update_database()\n",
    "                del segmentation_object\n",
    "                if autosave:\n",
    "                    self.database.save_all()\n",
    "        \n",
    "        if clear_tmp_data:\n",
    "            file_ids = self.database.get_file_ids_to_process(input_file_ids = None, process_tracker_key = 'segmentation_completed', overwrite = True)\n",
    "            segmentation_object = SegmentationObject(database = self.database, file_ids = file_ids, strategies = strategies)\n",
    "            segmentation_object.clear_all_tmp_data()\n",
    "\n",
    "\n",
    "    def reset_file_infos_if_new_strategy(self, strategies: List[ProcessingStrategy]) -> List:\n",
    "        new_strategy = False\n",
    "        for strategy in strategies:\n",
    "            processing_type = strategy().processing_type\n",
    "            strategy_name = strategy().strategy_name\n",
    "            matching_index = [key for key, column in self.database.file_infos.items() if f'{processing_type}_step' in key and strategy_name in column]\n",
    "            if len(matching_index) == 0:\n",
    "                new_strategy = True\n",
    "                break\n",
    "        if f'{processing_type}_completed' not in self.database.file_infos.keys():\n",
    "            column = [None] * len(self.database.file_infos['file_id'])\n",
    "        elif new_strategy:\n",
    "            column = [None] * len(self.database.file_infos['file_id'])\n",
    "        else:\n",
    "            column = self.database.file_infos[f'{processing_type}_completed']\n",
    "        return column\n",
    "\n",
    "\n",
    "    def create_batches(self, batch_size: int, file_ids: List[str], process_tracker_key: str, overwrite: bool) -> Optional[List]:\n",
    "            if batch_size <= 0:\n",
    "                raise ValueError('\"batch_size\" must be greater than 0!')\n",
    "            all_file_ids = self.database.get_file_ids_to_process(input_file_ids = file_ids, process_tracker_key = process_tracker_key, overwrite = overwrite)\n",
    "            if len(all_file_ids) == 0:\n",
    "                file_ids_per_batch = None\n",
    "            else:\n",
    "                file_ids_per_batch = []\n",
    "                while len(all_file_ids) > 0:\n",
    "                    if len(all_file_ids) >= batch_size:\n",
    "                        subsample = random.sample(all_file_ids, batch_size)\n",
    "                        for elem in subsample:\n",
    "                            all_file_ids.remove(elem)\n",
    "                        file_ids_per_batch.append(subsample)\n",
    "                    else:\n",
    "                        file_ids_per_batch.append(all_file_ids)\n",
    "                        all_file_ids = []\n",
    "            return file_ids_per_batch\n",
    "    \n",
    "\n",
    "    def postprocess(self, strategies: List[PostprocessingStrategy], segmentations_to_use: str, file_ids: Optional[List]=None, overwrite: bool=False) -> None:\n",
    "        if segmentations_to_use not in ['semantic', 'instance']:\n",
    "            raise ValueError('\"segmentations_to_use\" must be either \"semantic\" or \"instance\"')\n",
    "        else:\n",
    "            segmentations_to_use_dir = getattr(self.database, f'{segmentations_to_use}_segmentations_dir')\n",
    "            segmentations_present = False\n",
    "            for elem in segmentations_to_use_dir.iterdir():\n",
    "                if elem.is_file():\n",
    "                    segmentations_present = True\n",
    "                    break\n",
    "            if not segmentations_present:\n",
    "                if segmentations_to_use == 'semantic':\n",
    "                    error_message_line0 = f'It seems like there are no {segmentations_to_use} segmentations present in the corresponding directory.\\n'\n",
    "                    error_message_line1 = 'You need to run segmentations first, before you can postprocess them.'\n",
    "                    error_message = error_message_line0 + error_message_line1\n",
    "                    raise ValueError(error_message)\n",
    "                else: # has to be instance then\n",
    "                    error_message_line0 = f'It seems like there are no {segmentations_to_use} segmentations present in the corresponding directory.\\n'\n",
    "                    error_message_line1 = 'Did you mean to use \"semantic\" instead? Otherwise, please run the respective instance segmentations first.'\n",
    "                    error_message = error_message_line0 + error_message_line1\n",
    "                    raise ValueError(error_message)\n",
    "\n",
    "            file_ids = self.database.get_file_ids_to_process(input_file_ids = file_ids, process_tracker_key = 'postprocessing_completed', overwrite = overwrite)\n",
    "            for file_id in file_ids:\n",
    "                print(f'Postprocessing segmentations of file ID: {file_id} ({file_ids.index(file_id) + 1}/{len(file_ids)})')\n",
    "                postprocessing_object = PostprocessingObject(database = self.database, \n",
    "                                                             file_ids = [file_id], \n",
    "                                                             strategies = strategies, \n",
    "                                                             segmentations_to_use = segmentations_to_use)\n",
    "                postprocessing_object.run_all_strategies()\n",
    "                postprocessing_object.save_postprocessed_segmentations()\n",
    "                postprocessing_object.update_database()\n",
    "                del postprocessing_object\n",
    "\n",
    "\n",
    "    def quantify(self, strategies: List[QuantificationStrategy], file_ids: Optional[List]=None, overwrite: bool=False) -> None:\n",
    "        file_ids = self.database.get_file_ids_to_process(input_file_ids = file_ids, process_tracker_key = 'quantification_completed', overwrite = overwrite)\n",
    "        for file_id in file_ids:\n",
    "            print(f'Quantification of file ID: {file_id} ({file_ids.index(file_id) + 1}/{len(file_ids)})')\n",
    "            quantification_object = QuantificationObject(database = self.database, file_ids = [file_id], strategies = strategies)\n",
    "            quantification_object.run_all_strategies()\n",
    "            quantification_object.update_database()\n",
    "            del quantification_object\n",
    "            \n",
    "    \"\"\"        \n",
    "    def inspect(self, quantification_strategy_index: int=0, file_ids: Optional[List]=None, \n",
    "                area_roi_ids: Optional[List]=None, label_indices: Optional[List]=None, show: bool=True, save: bool=False) -> None:\n",
    "        from .inspection import InspectionObject\n",
    "        quantification_strategy_str = list(self.database.quantification_results.keys())[quantification_strategy_index]\n",
    "        file_ids_not_quantified = self.database.get_file_ids_to_process(input_file_ids = file_ids, process_tracker_key = 'quantification_completed', overwrite = False)\n",
    "        if file_ids == None:\n",
    "            file_ids = self.database.file_infos['file_id']\n",
    "        file_ids_quantified = [elem for elem in file_ids if elem not in file_ids_not_quantified]\n",
    "        for file_id in file_ids_quantified:\n",
    "            valid_area_roi_ids = self.database.area_rois_for_quantification[file_id]['all_planes'].keys()\n",
    "            if area_roi_ids == None:\n",
    "                tmp_area_roi_ids = valid_area_roi_ids\n",
    "            else:\n",
    "                tmp_area_roi_ids = [elem for elem in area_roi_ids if elem in valid_area_roi_ids]\n",
    "            for area_roi_id in tmp_area_roi_ids:\n",
    "                total_labels = self.database.quantification_results[quantification_strategy_str][file_id][area_roi_id]\n",
    "                if label_indices == None:\n",
    "                    tmp_label_indices = [elem for elem in range(total_labels)]\n",
    "                else:\n",
    "                    tmp_label_indices = [elem for elem in label_indices if elem < total_labels]\n",
    "                for label_index in tmp_label_indices:\n",
    "                    inspection_object = InspectionObject(database = self.database, file_id = file_id, area_roi_id = area_roi_id, label_index = label_index, show = show, save = save)\n",
    "                    inspection_object.run_all_inspection_steps()\n",
    "\n",
    "                    \n",
    "    def run_inspection(self, file_id: str, inspection_strategy):\n",
    "        from .inspection import InspectionStrategy\n",
    "        inspection_strategy.run(self.database, file_id)\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    def remove_file_id_from_project(self, file_id: str):\n",
    "        self.database.remove_file_id_from_project(file_id = file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9e336-20de-4170-8c6a-e2f84c83cff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd276781-772c-4968-942b-d5d7e682dd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcedb4ca-8cc4-4f4d-8f5d-0644a3a3fe1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c83d0-6a26-451b-befe-0f4a9f20bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907e380e-053d-4894-82b1-974d434528f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def listdir_nohidden(path: Path) -> List:\n",
    "    return [f for f in os.listdir(path) if f.startswith('.') == False]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
