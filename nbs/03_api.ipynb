{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6954a744-b3c7-4c91-8b51-4d6edd948d17",
   "metadata": {},
   "source": [
    "# API\n",
    "\n",
    "> This module defines the API of *findmycells*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133610c-c166-47e1-8923-d0b19fe65043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d254e8-c8ec-464e-9e20-14e3003ce2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from pathlib import Path, PosixPath\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from findmycells.configs import ProjectConfigs\n",
    "from findmycells.database import Database\n",
    "from findmycells.core import ProcessingStrategy\n",
    "from findmycells.preprocessing.specs import PreprocessingStrategy, PreprocessingObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c83d0-6a26-451b-befe-0f4a9f20bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472d628-c4b0-40a3-9ecb-52626f2b3ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class API:\n",
    "    \n",
    "    def __init__(self, project_root_dir: PosixPath) -> None:\n",
    "        assert type(project_root_dir) == PosixPath, '\"project_root_dir\" must be pathlib.Path object referring to an existing directory.'\n",
    "        assert project_root_dir.is_dir(), '\"project_root_dir\" must be pathlib.Path object referring to an existing directory.'\n",
    "        self.project_configs = ProjectConfigs(root_dir = project_root_dir)\n",
    "        self.database = Database(project_configs = self.project_configs)\n",
    "        \n",
    "        \n",
    "    def update_database_with_current_source_files(self, skip_checking: bool=False) -> None:\n",
    "        self.database.compute_file_infos(skip_checking = skip_checking)\n",
    "        \n",
    "        \n",
    "    def preprocess(self,\n",
    "                   strategies: List[PreprocessingStrategy],\n",
    "                   strategy_configs: Optional[List[Dict]]=None,\n",
    "                   processing_configs: Optional[Dict]=None,\n",
    "                   microscopy_reader_configs: Optional[Dict]=None,\n",
    "                   roi_reader_configs: Optional[Dict]=None,\n",
    "                   file_ids: Optional[List[str]]=None\n",
    "                  ) -> None:\n",
    "        processing_step_id = 'preprocessing'\n",
    "        strategy_configs, processing_configs, file_ids = self._assert_and_update_input(processing_step_id = processing_step_id,\n",
    "                                                                                       strategies = strategies,\n",
    "                                                                                       strategy_configs = strategy_configs,\n",
    "                                                                                       processing_configs = processing_configs,\n",
    "                                                                                       file_ids = file_ids)\n",
    "        microscopy_reader_configs = self._assert_and_update_reader_configs_input(reader_type = 'microscopy_images', reader_configs = microscopy_reader_configs)\n",
    "        roi_reader_configs = self._assert_and_update_reader_configs_input(reader_type = 'rois', reader_configs = roi_reader_configs)\n",
    "        for file_id in tqdm(file_ids, display = processing_configs['show_progress']):\n",
    "            preprocessing_object = PreprocessingObject()\n",
    "            preprocessing_object.prepare_for_processing(file_ids = [file_id], database = self.database)\n",
    "            preprocessing_object.load_image_and_rois(microscopy_reader_configs = microscopy_reader_configs, roi_reader_configs = roi_reader_configs)\n",
    "            preprocessing_object.run_all_strategies(strategies = strategies, strategy_configs = strategy_configs)\n",
    "            preprocessing_object.save_preprocessed_images_on_disk()\n",
    "            preprocessing_object.save_preprocessed_rois_in_database()\n",
    "            preprocessing_object.update_database()\n",
    "            del preprocessing_object\n",
    "            if processing_configs['autosave'] == True:\n",
    "                self.save_status()\n",
    "                self.load_status()\n",
    "                \n",
    "                \n",
    "    def _assert_and_update_reader_configs_input(self, reader_type: str, reader_configs: Optional[Dict]) -> Dict:            \n",
    "        if reader_configs == None:\n",
    "            if hasattr(self.project_configs, reader_type) == False:\n",
    "                self.project_configs.add_reader_configs(reader_type = reader_type)\n",
    "            reader_configs = getattr(self.project_configs, reader_type)\n",
    "        else:\n",
    "            assert type(reader_configs) == dict, f'\"reader_configs\" (data type: {reader_type}) has to be a dictionary!'\n",
    "            default_configs = self.project_configs.default_configs_of_available_data_readers[reader_type]\n",
    "            default_configs.assert_user_input(user_input = reader_configs)\n",
    "            reader_configs = default_configs.fill_user_input_with_defaults_where_needed(user_input = reader_configs)\n",
    "            self.project_configs.add_reader_configs(reader_type = reader_type, reader_configs = reader_configs)\n",
    "        return reader_configs\n",
    "\n",
    "    \n",
    "    def save_status(self) -> None:\n",
    "        date = f'{datetime.now():%Y_%m_%d}'\n",
    "        dbase_filename = f'{date}_findmycells_database.dbase'\n",
    "        self._save_attr_to_disk(attr_id = 'database', filename = dbase_filename, child_attr_ids_to_del = ['project_configs'])\n",
    "        configs_filename = f'{date}_findmycells_project.configs'\n",
    "        self._save_attr_to_disk(attr_id = 'project_configs', filename = configs_filename, child_attr_ids_to_del = ['available_processing_modules'])\n",
    "        \n",
    "    \n",
    "    def _save_attr_to_disk(self, attr_id: str, filename: str, child_attr_ids_to_del: List[str]) -> None:\n",
    "        filepath = self.project_configs.root_dir.joinpath(filename)\n",
    "        attribute_to_save = getattr(self, attr_id)\n",
    "        for attr_id_to_del in child_attr_ids_to_del:\n",
    "            delattr(attribute_to_save, attr_id_to_del)\n",
    "        filehandler = open(filepath, 'wb')\n",
    "        pickle.dump(attribute_to_save, filehandler)\n",
    "\n",
    "        \n",
    "    def _load_object_from_filepath(self, filepath: PosixPath) -> Union[Database, ProjectConfigs]:\n",
    "        filehandler = open(filepath, 'rb')\n",
    "        loaded_object = pickle.load(filehandler)\n",
    "        return loaded_object\n",
    "\n",
    "        \n",
    "    def load_status(self,\n",
    "                    project_configs_filepath: Optional[PosixPath]=None,\n",
    "                    database_filepath: Optional[PosixPath]=None\n",
    "                   ) -> None:\n",
    "        if project_configs_filepath != None:\n",
    "            assert type(project_configs_filepath) == PosixPath, '\"project_configs_filepath\" must be pathlib.Path object referring to a .configs file.'\n",
    "            assert project_configs_filepath.suffix == '.configs', '\"project_configs_filepath\" must be pathlib.Path object referring to a .configs file.'\n",
    "        else:\n",
    "            project_configs_filepath = self._look_for_latest_status_file_in_dir(suffix = '.configs', dir_path = self.project_configs.root_dir)\n",
    "        if database_filepath != None:\n",
    "            assert type(database_filepath) == PosixPath, '\"database_filepath\" must be pathlib.Path object referring to a .dbase file'\n",
    "            assert database_filepath.suffix == '.dbase', '\"database_filepath\" must be pathlib.Path object referring to a .dbase file'\n",
    "        else:\n",
    "            database_filepath = self._look_for_latest_status_file_in_dir(suffix = '.dbase', dir_path = self.project_configs.root_dir)\n",
    "        if hasattr(self, 'project_configs'):\n",
    "            delattr(self, 'project_configs')\n",
    "        if hasattr(self, 'database'):\n",
    "            delattr(self, 'database')\n",
    "        self.project_configs = self._load_object_from_filepath(filepath = project_configs_filepath)\n",
    "        self.project_configs.load_available_processing_modules()\n",
    "        self.database = self._load_object_from_filepath(filepath = database_filepath)\n",
    "        setattr(self.database, 'project_configs', self.project_configs)\n",
    "        \n",
    "\n",
    "    def split_file_ids_into_batches(self, file_ids: List[str], batch_size: int) -> List[List[str]]:\n",
    "        if len(file_ids) % batch_size == 0:\n",
    "            total_batches = int(len(file_ids) / batch_size)\n",
    "        else:\n",
    "            total_batches = int(len(file_ids) / batch_size) + 1\n",
    "        file_ids_per_batch = []\n",
    "        for batch in range(total_batches):\n",
    "            if len(file_ids) >= batch_size:\n",
    "                sampled_file_ids = random.sample(file_ids, batch_size)\n",
    "            else:\n",
    "                sampled_file_ids = file_ids.copy()\n",
    "            file_ids_per_batch.append(sampled_file_ids)\n",
    "            for elem in sampled_file_ids:\n",
    "                file_ids.remove(elem)    \n",
    "        return file_ids_per_batch\n",
    "\n",
    "\n",
    "    def _look_for_latest_status_file_in_dir(self, suffix: str, dir_path: PosixPath) -> PosixPath:\n",
    "        matching_filepaths = [filepath for filepath in dir_path.iterdir() if filepath.suffix == suffix]\n",
    "        if len(matching_filepaths) == 0:\n",
    "            raise FileNotFoundError(f'Could not find a \"{suffix}\" file in {dir_path}. Consider specifying the exact filepath!')\n",
    "        else:\n",
    "            date_strings = [filepath.name[:10] for filepath in matching_filepaths]\n",
    "            dates = [datetime.strptime(date_str, '%Y_%m_%d') for date_str in date_strings]\n",
    "            latest_date = max(dates)\n",
    "            filepath_idx = dates.index(latest_date)\n",
    "            latest_status_filepath = matching_filepaths[filepath_idx]\n",
    "        return latest_status_filepath        \n",
    "        \n",
    "        \n",
    "    def _assert_and_update_input(self, \n",
    "                                 processing_step_id: str,\n",
    "                                 strategies: List[PreprocessingStrategy],\n",
    "                                 strategy_configs: Optional[List[Dict]],\n",
    "                                 processing_configs: Optional[Dict],\n",
    "                                 file_ids: Optional[List[str]]\n",
    "                                ) -> Tuple[List[Dict], Dict, List[str]]:\n",
    "        self._assert_processing_step_input(processing_step_id = processing_step_id,\n",
    "                                           strategies = strategies,\n",
    "                                           strategy_configs = strategy_configs,\n",
    "                                           processing_configs = processing_configs,\n",
    "                                           file_ids = file_ids)\n",
    "        strategy_configs = self._fill_strategy_configs_with_defaults_where_needed(strategies, strategy_configs)\n",
    "        if processing_configs == None:\n",
    "            if hasattr(self.project_configs, processing_step_id) == False:\n",
    "                self.project_configs.add_processing_step_configs(processing_step_id = processing_step_id)\n",
    "            processing_configs = getattr(self.project_configs, processing_step_id)\n",
    "        processing_configs = self._fill_processing_configs_with_defaults_where_needed(processing_step_id, processing_configs)\n",
    "        self.project_configs.add_processing_step_configs(processing_step_id, configs = processing_configs)\n",
    "        file_ids = self.database.get_file_ids_to_process(input_file_ids = file_ids,\n",
    "                                                         processing_step_id = processing_step_id,\n",
    "                                                         overwrite = processing_configs['overwrite'])\n",
    "        return strategy_configs, processing_configs, file_ids\n",
    "            \n",
    "        \n",
    "    def _assert_processing_step_input(self, \n",
    "                                      processing_step_id: str,\n",
    "                                      strategies: List[PreprocessingStrategy],\n",
    "                                      strategy_configs: Optional[List[Dict]],\n",
    "                                      processing_configs: Optional[Dict],\n",
    "                                      file_ids: Optional[List[str]]\n",
    "                                     ) -> None:\n",
    "        assert type(strategies) == list, '\"strategies\" has to ba a list of ProcessingStrategy classes of the respective processing step!'\n",
    "        if strategy_configs != None:\n",
    "            assert type(strategy_configs) == list, '\"strategy_configs\" has to be None or a list of the same length as \"strategies\"!'\n",
    "            assert len(strategy_configs) == len(strategies), '\"strategy_configs\" has to be None or a list of the same length as \"strategies\"!'\n",
    "        else:\n",
    "            strategy_configs = [None] * len(strategies)\n",
    "        available_strategies = self.project_configs.available_processing_strategies[processing_step_id]\n",
    "        for strat, config in zip(strategies, strategy_configs):\n",
    "            assert strat in available_strategies, f'{strat} is not an available strategy for {processing_step_id}!'\n",
    "            if config != None:\n",
    "                strat().default_configs.assert_user_input(user_input = config)\n",
    "        if processing_configs != None:\n",
    "            processing_obj = self.project_configs.available_processing_objects[processing_step_id]()\n",
    "            processing_obj.default_configs.assert_user_input(user_input = processing_configs)\n",
    "        if file_ids != None:\n",
    "            assert type(file_ids) == list, '\"file_ids\" has to be a list of strings referring to file_ids in the database!'\n",
    "            for elem in file_ids:\n",
    "                assert elem in self.database.file_infos['file_id'], f'{elem} is not a valid file_id!'\n",
    "        \n",
    "        \n",
    "    def _fill_processing_configs_with_defaults_where_needed(self,\n",
    "                                                            processing_step_id: str,\n",
    "                                                            processing_configs: Dict\n",
    "                                                           ) -> Dict:\n",
    "        processing_obj = self.project_configs.available_processing_objects[processing_step_id]()\n",
    "        return processing_obj.default_configs.fill_user_input_with_defaults_where_needed(user_input = processing_configs)                                              \n",
    "             \n",
    "        \n",
    "    def _fill_strategy_configs_with_defaults_where_needed(self,\n",
    "                                                          strategies: List[ProcessingStrategy],\n",
    "                                                          strategy_configs: Optional[List[Dict]]\n",
    "                                                         ) -> List[Dict]:\n",
    "        all_final_configs = []\n",
    "        if strategy_configs == None:\n",
    "            for strat in strategies:\n",
    "                default_configs = strat().default_configs.fill_user_input_with_defaults_where_needed(user_input = {})\n",
    "                all_final_configs.append(default_configs)\n",
    "        else:\n",
    "            for strat, configs in zip(strategies, strategy_configs):\n",
    "                full_configs = strat().default_configs.fill_user_input_with_defaults_where_needed(user_input = configs)\n",
    "                all_final_configs.append(full_configs)\n",
    "        return all_final_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac09343-eb80-441c-a042-5db978cff7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
